{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# MNIST dataset\n",
    "train_data = torchvision.datasets.MNIST(root='data',train=True,\n",
    "                                        transform=img_transform, download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='data',train=False,\n",
    "                                       transform=img_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "im_size = 28\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-3\n",
    "\n",
    "input_dim = im_size * im_size\n",
    "hidden_dim = 256\n",
    "latent_dim = 75\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2onehot(idx, n):\n",
    "\n",
    "    assert idx.shape[1] == 1\n",
    "    assert torch.max(idx).item() < n\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx.data, 1)\n",
    "\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim + n_classes, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim + n_classes]\n",
    "\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "\n",
    "        # latent parameters\n",
    "        mean = self.mu(hidden)\n",
    "        # mean is of shape [batch_size, latent_dim]\n",
    "        log_var = self.var(hidden)\n",
    "        # log_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return mean, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the size of output (in case of MNIST 28 * 28).\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim + n_classes, hidden_dim)\n",
    "        self.hidden_to_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, latent_dim + num_classes]\n",
    "        x = F.relu(self.latent_to_hidden(x))\n",
    "        # x is of shape [batch_size, hidden_dim]\n",
    "        generated_x = F.sigmoid(self.hidden_to_out(x))\n",
    "        # x is of shape [batch_size, output_dim]\n",
    "\n",
    "        return generated_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, n_classes):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            latent_dim: A integer indicating the latent size.\n",
    "            n_classes: A integer indicating the number of classes. (dimension of one-hot representation of labels)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim, n_classes)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y), dim=1)\n",
    "        # encode\n",
    "        z_mu, z_var = self.encoder(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "        z = torch.cat((x_sample, y), dim=1)\n",
    "\n",
    "        # decode\n",
    "        generated_x = self.decoder(z)\n",
    "\n",
    "        return generated_x, z_mu, z_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = CVAE(input_dim, hidden_dim, latent_dim, n_classes)\n",
    "model.to(device)\n",
    "#optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x, reconstructed_x, mean, log_var):\n",
    "    # reconstruction loss\n",
    "    RCL = F.binary_cross_entropy(reconstructed_x, x, size_average=False)\n",
    "    # kl divergence loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return RCL + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # convert y into one-hot encoding\n",
    "        y = idx2onehot(y.view(-1, 1), 10)\n",
    "        \n",
    "        y = y.to(device)\n",
    "        \n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "        # loss\n",
    "        loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        break\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # reshape the data\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = x.to(device)\n",
    "\n",
    "            # convert y into one-hot encoding\n",
    "            y = idx2onehot(y.view(-1, 1), 10)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            reconstructed_x, z_mu, z_var = model(x, y)\n",
    "\n",
    "            # loss\n",
    "            loss = calculate_loss(x, reconstructed_x, z_mu, z_var)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.84, Test Loss: 367.62\n",
      "Epoch 100, Train Loss: 0.43, Test Loss: 191.78\n",
      "Epoch 200, Train Loss: 0.36, Test Loss: 164.36\n",
      "Epoch 300, Train Loss: 0.32, Test Loss: 153.59\n",
      "Epoch 400, Train Loss: 0.31, Test Loss: 144.63\n",
      "Epoch 500, Train Loss: 0.29, Test Loss: 138.36\n",
      "Epoch 600, Train Loss: 0.29, Test Loss: 133.37\n",
      "Epoch 700, Train Loss: 0.29, Test Loss: 129.55\n",
      "Epoch 800, Train Loss: 0.27, Test Loss: 126.31\n",
      "Epoch 900, Train Loss: 0.26, Test Loss: 123.72\n",
      "Epoch 1000, Train Loss: 0.26, Test Loss: 121.73\n",
      "Epoch 1100, Train Loss: 0.25, Test Loss: 119.89\n",
      "Epoch 1200, Train Loss: 0.25, Test Loss: 118.30\n",
      "Epoch 1300, Train Loss: 0.25, Test Loss: 116.62\n",
      "Epoch 1400, Train Loss: 0.26, Test Loss: 115.19\n",
      "Epoch 1500, Train Loss: 0.24, Test Loss: 114.39\n",
      "Epoch 1600, Train Loss: 0.23, Test Loss: 113.33\n",
      "Epoch 1700, Train Loss: 0.23, Test Loss: 112.42\n",
      "Epoch 1800, Train Loss: 0.24, Test Loss: 111.68\n",
      "Epoch 1900, Train Loss: 0.24, Test Loss: 111.12\n",
      "Epoch 2000, Train Loss: 0.24, Test Loss: 111.02\n",
      "Epoch 2100, Train Loss: 0.23, Test Loss: 110.03\n",
      "Epoch 2200, Train Loss: 0.22, Test Loss: 109.41\n",
      "Epoch 2300, Train Loss: 0.23, Test Loss: 109.16\n",
      "Epoch 2400, Train Loss: 0.23, Test Loss: 108.57\n",
      "Epoch 2500, Train Loss: 0.24, Test Loss: 108.05\n",
      "Epoch 2600, Train Loss: 0.24, Test Loss: 107.81\n",
      "Epoch 2700, Train Loss: 0.23, Test Loss: 107.41\n",
      "Epoch 2800, Train Loss: 0.22, Test Loss: 106.90\n",
      "Epoch 2900, Train Loss: 0.23, Test Loss: 106.84\n",
      "Epoch 3000, Train Loss: 0.22, Test Loss: 106.63\n",
      "Epoch 3100, Train Loss: 0.23, Test Loss: 106.28\n",
      "Epoch 3200, Train Loss: 0.23, Test Loss: 106.05\n",
      "Epoch 3300, Train Loss: 0.23, Test Loss: 105.78\n",
      "Epoch 3400, Train Loss: 0.22, Test Loss: 105.87\n",
      "Epoch 3500, Train Loss: 0.22, Test Loss: 105.24\n",
      "Epoch 3600, Train Loss: 0.23, Test Loss: 105.35\n",
      "Epoch 3700, Train Loss: 0.23, Test Loss: 105.09\n",
      "Epoch 3800, Train Loss: 0.21, Test Loss: 104.99\n",
      "Epoch 3900, Train Loss: 0.23, Test Loss: 104.80\n",
      "Epoch 4000, Train Loss: 0.22, Test Loss: 104.63\n",
      "Epoch 4100, Train Loss: 0.22, Test Loss: 104.40\n",
      "Epoch 4200, Train Loss: 0.22, Test Loss: 104.46\n",
      "Epoch 4300, Train Loss: 0.22, Test Loss: 104.43\n",
      "Epoch 4400, Train Loss: 0.22, Test Loss: 104.23\n",
      "Epoch 4500, Train Loss: 0.23, Test Loss: 104.39\n",
      "Epoch 4600, Train Loss: 0.23, Test Loss: 104.04\n",
      "Epoch 4700, Train Loss: 0.22, Test Loss: 103.85\n",
      "Epoch 4800, Train Loss: 0.22, Test Loss: 103.85\n",
      "Epoch 4900, Train Loss: 0.22, Test Loss: 103.46\n"
     ]
    }
   ],
   "source": [
    "for e in range(5000):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(train_data)\n",
    "    test_loss /= len(test_data)\n",
    "    \n",
    "    if e%100 == 0:\n",
    "        print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'cvae_mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from latent distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(root='data',train=False,\n",
    "                                       transform=img_transform, download=True)\n",
    "\n",
    "ind_ones = torch.where((test_data.targets == 1))\n",
    "labels_ones = test_data.targets[ind_ones]\n",
    "data_ones = test_data.data[ind_ones].float()\n",
    "ones_dataset = torch.utils.data.TensorDataset(data_ones, labels_ones)\n",
    "ones_loader = torch.utils.data.DataLoader(dataset=ones_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "ind_sevens = torch.where((test_data.targets == 7))\n",
    "labels_sevens = test_data.targets[ind_sevens]\n",
    "data_sevens = test_data.data[ind_sevens].float()\n",
    "sevens_dataset = torch.utils.data.TensorDataset(data_sevens, labels_sevens)\n",
    "sevens_loader = torch.utils.data.DataLoader(dataset=sevens_dataset,\n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batch_data_from_loader(data_loader):\n",
    "    list_imgs = []\n",
    "    list_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in ones_loader:\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = x.to(device)\n",
    "            y = idx2onehot(y.view(-1, 1), 10)\n",
    "            y = y.to(device)\n",
    "\n",
    "            list_imgs.append(x)\n",
    "            list_labels.append(y)    \n",
    "\n",
    "    return list_imgs, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_imgs_ones, list_labels_ones = return_batch_data_from_loader(ones_loader)\n",
    "list_imgs_sevens, list_labels_sevens = return_batch_data_from_loader(sevens_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_latent_parameters(model, batch_data, labels):\n",
    "    model.eval()\n",
    "    paramterers = {}\n",
    "    _, z_mu, z_var = model(batch_data, labels)\n",
    "    \n",
    "    std = torch.exp(z_var / 2)\n",
    "    eps = torch.randn_like(std)\n",
    "    \n",
    "    paramterers['mu'] = z_mu\n",
    "    paramterers['std'] = std\n",
    "    paramterers['eps'] = eps\n",
    "     \n",
    "    return z_mu, std, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_for_normal = list_imgs_ones[:-3]\n",
    "ones_labels_for_normal = list_labels_ones[:-3]\n",
    "\n",
    "ones_for_normal_starts = ones_for_normal[:3]\n",
    "ones_labels_for_normal_starts = ones_labels_for_normal[:3]\n",
    "\n",
    "ones_for_normal_ends =  ones_for_normal[3:]\n",
    "ones_labels_for_normal_ends = ones_labels_for_normal[3:]\n",
    "\n",
    "ones_for_anomaly = list_imgs_ones[-3:]\n",
    "ones_labels_for_anomaly = list_labels_ones[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sevens_for_anomaly = list_imgs_sevens[:2]\n",
    "sevens_labels_for_anomaly = list_labels_sevens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_numbers(y1, y2, x):\n",
    "    return (y2 - y1)*x + y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eromanenkova/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "seq_len = 128\n",
    "ts = torch.linspace(0, 1, seq_len)\n",
    "imgs_series = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    starts = ones_for_normal_starts[i]    \n",
    "    starts_labels = ones_labels_for_normal_starts[i]\n",
    "    ends = ones_for_normal_ends[i]\n",
    "    ends_labels = ones_labels_for_normal_ends[i]\n",
    "    \n",
    "    labels_ones = ones_labels_for_normal[0][0]\n",
    "    \n",
    "    z_mu_1, std_1, eps_1 = return_latent_parameters(model, starts, starts_labels)\n",
    "    z_mu_2, std_2, eps_2 = return_latent_parameters(model, ends, ends_labels)\n",
    "    \n",
    "    for j in range(batch_size):\n",
    "        \n",
    "        z_mu_1_j = z_mu_1[j]\n",
    "        z_mu_2_j = z_mu_2[j]\n",
    "        std_1_j = std_1[j]\n",
    "        std_2_j = std_2[j]\n",
    "        eps_1_j = eps_1[j]\n",
    "        eps_2_j = eps_2[j]\n",
    "        \n",
    "        images_serie = []\n",
    "        \n",
    "        for k in range(seq_len):\n",
    "            \n",
    "            coef = ts[k]\n",
    "            z_mu_new = interpolate_numbers(z_mu_1_j, z_mu_2_j, coef)\n",
    "            std_new = interpolate_numbers(std_1_j, std_2_j, coef)\n",
    "            eps_new = interpolate_numbers(eps_1_j, eps_2_j, coef)\n",
    "            \n",
    "            x_sample = z_mu_new.add(std_new.mul(eps_new))\n",
    "            \n",
    "            z_new = torch.cat((x_sample, labels_ones))\n",
    "            \n",
    "            images_serie.append(model.decoder(z_new))\n",
    "                            \n",
    "        imgs_series.append(images_serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACoRJREFUeJzt3U+IXfd5h/HnWyvZOFnIaGyEY1dpMKWmUKUMouBSXIKDk42cRUq0CCoElEUMCWRR4028KZjSJO2iBJRaRIXEIZC41sK0ESbgBkrwyJhYrtrYGCVRJKQxXsRZBdtvF3MUJvaM5vrec/+Y9/nAcO8998ycl4ueuX9Hv1QVkvr5g2UPIGk5jF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpvYt8mAHDhyoQ4cOLfKQUivnzp17tarWJtl3pviT3A/8M3AT8K9V9eiN9j906BAbGxuzHFLSDST5+aT7Tv2wP8lNwL8AnwDuBo4luXvanydpsWZ5zn8EeLmqXqmq3wLfBY6OM5akeZsl/tuBX267fGnY9nuSnEiykWRjc3NzhsNJGtMs8WeHbe/4++CqOllV61W1vrY20esQkhZglvgvAXdsu/wh4PJs40halFnifxa4K8mHk7wf+AxwZpyxJM3b1G/1VdUbSR4E/pOtt/pOVdWLo00maa5mep+/qp4CnhppFkkL5Md7paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpmZapTfJReB14E3gjapaH2MoSfM3U/yDv66qV0f4OZIWyIf9UlOzxl/AD5OcS3JijIEkLcasD/vvqarLSW4Fzib536p6ZvsOwy+FEwB33nnnjIeTNJaZ7vmr6vJweg14Ajiywz4nq2q9qtbX1tZmOZykEU0df5Kbk3zw+nng48D5sQaTNF+zPOy/DXgiyfWf852q+o9RppI0d1PHX1WvAH824iySFsi3+qSmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2pqz/iTnEpyLcn5bdtuSXI2yUvD6f75jilpbJPc838LuP9t2x4Cnq6qu4Cnh8uS3kP2jL+qngFee9vmo8Dp4fxp4IGR55I0Z9M+57+tqq4ADKe3jjeSpEWY+wt+SU4k2Uiysbm5Oe/DSZrQtPFfTXIQYDi9ttuOVXWyqtaran1tbW3Kw0ka27TxnwGOD+ePA0+OM46kRZnkrb7Hgf8G/jjJpSSfAx4F7kvyEnDfcFnSe8i+vXaoqmO7XPWxkWeRtEB+wk9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rK+KWm9ow/yakk15Kc37btkSS/SvL88PXJ+Y4paWyT3PN/C7h/h+1fr6rDw9dT444lad72jL+qngFeW8AskhZoluf8Dyb56fC0YP9oE0laiGnj/wbwEeAwcAX46m47JjmRZCPJxubm5pSHkzS2qeKvqqtV9WZVvQV8Ezhyg31PVtV6Va2vra1NO6ekkU0Vf5KD2y5+Cji/276SVtO+vXZI8jhwL3AgySXgK8C9SQ4DBVwEPj/HGSXNwZ7xV9WxHTY/NodZJC2Qn/CTmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pqT3jT3JHkh8luZDkxSRfHLbfkuRskpeG0/3zH1fSWCa5538D+HJV/QnwF8AXktwNPAQ8XVV3AU8PlyW9R+wZf1VdqarnhvOvAxeA24GjwOlht9PAA/MaUtL43tVz/iSHgI8CPwFuq6orsPULArh17OEkzc/E8Sf5APB94EtV9et38X0nkmwk2djc3JxmRklzMFH8Sd7HVvjfrqofDJuvJjk4XH8QuLbT91bVyapar6r1tbW1MWaWNIJJXu0P8Bhwoaq+tu2qM8Dx4fxx4Mnxx5M0L/sm2Oce4LPAC0meH7Y9DDwKfC/J54BfAJ+ez4iS5mHP+Kvqx0B2ufpj444jaVH8hJ/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtN7Rl/kjuS/CjJhSQvJvnisP2RJL9K8vzw9cn5jytpLPsm2OcN4MtV9VySDwLnkpwdrvt6Vf3j/MaTNC97xl9VV4Arw/nXk1wAbp/3YJLm6109509yCPgo8JNh04NJfprkVJL9u3zPiSQbSTY2NzdnGlbSeCaOP8kHgO8DX6qqXwPfAD4CHGbrkcFXd/q+qjpZVetVtb62tjbCyJLGMFH8Sd7HVvjfrqofAFTV1ap6s6reAr4JHJnfmJLGNsmr/QEeAy5U1de2bT+4bbdPAefHH0/SvEzyav89wGeBF5I8P2x7GDiW5DBQwEXg83OZUNJcTPJq/4+B7HDVU+OPI2lR/ISf1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS02lqhZ3sGQT+Pm2TQeAVxc2wLuzqrOt6lzgbNMac7Y/rKqJ/r+8hcb/joMnG1W1vrQBbmBVZ1vVucDZprWs2XzYLzVl/FJTy47/5JKPfyOrOtuqzgXONq2lzLbU5/ySlmfZ9/ySlmQp8Se5P8n/JXk5yUPLmGE3SS4meWFYeXhjybOcSnItyflt225JcjbJS8PpjsukLWm2lVi5+QYrSy/1tlu1Fa8X/rA/yU3Az4D7gEvAs8CxqvqfhQ6yiyQXgfWqWvp7wkn+CvgN8G9V9afDtn8AXquqR4dfnPur6u9WZLZHgN8se+XmYUGZg9tXlgYeAP6WJd52N5jrb1jC7baMe/4jwMtV9UpV/Rb4LnB0CXOsvKp6BnjtbZuPAqeH86fZ+sezcLvMthKq6kpVPTecfx24vrL0Um+7G8y1FMuI/3bgl9suX2K1lvwu4IdJziU5sexhdnDbsGz69eXTb13yPG+358rNi/S2laVX5rabZsXrsS0j/p1W/1mltxzuqao/Bz4BfGF4eKvJTLRy86LssLL0Sph2xeuxLSP+S8Ad2y5/CLi8hDl2VFWXh9NrwBOs3urDV68vkjqcXlvyPL+zSis377SyNCtw263SitfLiP9Z4K4kH07yfuAzwJklzPEOSW4eXoghyc3Ax1m91YfPAMeH88eBJ5c4y+9ZlZWbd1tZmiXfdqu24vVSPuQzvJXxT8BNwKmq+vuFD7GDJH/E1r09bC1i+p1lzpbkceBetv7q6yrwFeDfge8BdwK/AD5dVQt/4W2X2e5l66Hr71Zuvv4ce8Gz/SXwX8ALwFvD5ofZen69tNvuBnMdYwm3m5/wk5ryE35SU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNfX/kRYDFmxfOGcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from celluloid import Camera\n",
    "\n",
    "fig = plt.figure()\n",
    "camera = Camera(fig)\n",
    "\n",
    "for ind in range(seq_len):\n",
    "    plt.imshow(imgs_series[1][ind].view(28,28).detach().cpu().numpy())\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('animation.gif', writer = 'imagemagick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
